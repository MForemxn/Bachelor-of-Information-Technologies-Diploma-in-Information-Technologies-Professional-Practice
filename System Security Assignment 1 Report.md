# Fake/Artificial data detection - Latest (image/video) Deepfake detection methods
# Drafts
## Draft 1
### Fake/Artificial Data Detection: Latest Image and Video Deepfake Detection Methods

The rapid advancement of artificial intelligence has enabled the creation of increasingly sophisticated deepfakes—synthetic media that can convincingly simulate real individuals in fabricated scenarios. These AI-generated forgeries pose substantial threats to media integrity, privacy, and public trust. As deepfake technology becomes more accessible and realistic, the development of robust detection methods has become critical for maintaining digital media security. The technological arms race between deepfake creators and detectors continues to accelerate, with detection methods constantly evolving to keep pace with increasingly sophisticated forgery techniques. This report explores the current landscape of deepfake detection methods, focusing particularly on image and video analysis techniques that leverage recent advances in deep learning and computer vision.

#### Understanding Deepfake Technology

Deepfakes originated from the combination of "deep learning" and "fake," representing AI-generated content that can convincingly manipulate existing media. These technologies primarily employ sophisticated neural networks to create or alter visual and audio content in ways that appear authentic to human observers. The creation of deepfake content regularly involves three main approaches. The first method is face synthesis, which generates an entirely new face with specified characteristics and behaviours[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). The second approach is face swapping, which replaces only the face of the target person while maintaining the original facial expressions[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). The third method focuses on lip syncing, manipulating just the lip region to make subjects appear to say things they never actually said[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf).

The detection methods for identifying such manipulated content are generally formulated as frame-level binary classification problems. These approaches can be broadly categorised into three types: those focusing on physical or physiological inconsistencies in deepfake videos, those leveraging signal-level artefacts introduced during the creation process, and data-driven approaches that employ deep neural networks (DNNs) trained on genuine and fake videos[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). Each of these methods offers distinctive advantages but also faces specific limitations related to deepfake dataset quality, social media compression effects, and the continued evolution of forgery techniques.

Deepfake technology fundamentally relies on Generative Adversarial Networks (GANs), which consist of two competing neural networks—a generator that creates synthetic samples and a discriminator that attempts to distinguish between real and fake samples[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). This adversarial training process gradually improves the quality of the generated content until it becomes increasingly difficult to distinguish from authentic media. The proliferation of such sophisticated technology has notable implications for information integrity, prompting intensive research into effective countermeasures.

#### Image-Based Deepfake Detection Methods

Recent advances in image-based deepfake detection have focused on identifying artifacts and inconsistencies that are typically imperceptible to human observers but detectable through algorithmic analysis. Convolutional Neural Networks (CNNs) have emerged as particularly powerful tools for this purpose, with various architectures demonstrating promising results across different datasets and manipulation techniques. The comprehensive review published in 2024 indicates that CNN-based approaches remain the most commonly employed deep learning method for deepfake detection, owing to their effectiveness in identifying spatial inconsistencies in manipulated images[4](https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1520).

One significant branch of research involves examining facial inconsistencies in manipulated images. Xin Yang et al. developed a system that detects deepfakes by identifying inconsistent head poses, leveraging the fact that deepfake algorithms often create mismatched facial landmarks when swapping faces[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). This approach exploits a fundamental limitation in many deepfake generation methods, which focus primarily on the central facial region while neglecting to properly align head orientation vectors. By analyzing the distribution of cosine distances between head orientation vectors, this method effectively identifies manipulated content that exhibits subtle but detectable geometric inconsistencies.

Researchers have also explored novel approaches using GAN-based frameworks specifically designed for deepfake detection. The MRI-GAN framework utilises perceptual differences in images to detect synthesised content, achieving 74% test accuracy using Structural Similarity Index Measurement (SSIM) for perceptual difference analysis[3](https://github.com/pratikpv/mri_gan_deepfake). This approach generates what the researchers call an "MRI" of the input image, which reveals artefacts in deepfake images by highlighting regions of synthesised pixels. When applied to genuine images, the MRI output is simply a black image, providing a straightforward visual indicator of authenticity[3](https://github.com/pratikpv/mri_gan_deepfake). This perceptual approach offers an innovative perspective that complements traditional classification-based detection methods.

Feature extraction algorithms have also demonstrated effectiveness in deepfake detection workflows. Faten F Kharbat et al. proposed a method using Support Vector Machine (SVM) regression that analyzes feature points extracted from videos[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). Their system exploits inconsistencies by extracting feature points using traditional edge feature detectors, including HOG, ORB, BRISK, KAZE, SURF, and FAST. When tested on a dataset of 98 videos (half fake, half real), their approach achieved 95% accuracy using the HOG feature extraction algorithm[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). This demonstrates that traditional computer vision techniques, when appropriately applied, can complement deep learning approaches in deepfake detection.

#### Video-Based Deepfake Detection Methods

Video deepfakes present additional challenges compared to static images, as they must maintain consistency across multiple frames. However, this temporal dimension also provides additional opportunities for detection based on inconsistencies over time. Recent research has capitalised on these temporal aspects to develop increasingly effective detection methods for manipulated video content. The comprehensive survey published in January 2025 highlights that state-of-the-art detection techniques now incorporate both spatial and temporal analysis to identify inconsistencies across video sequences[2](https://pubmed.ncbi.nlm.nih.gov/39968137/).

Temporal pattern analysis represents a particularly effective approach for video deepfake detection. Mohammed A. Younus et al. explored techniques that analyse temporal patterns in videos, identifying inconsistencies that emerge across consecutive frames[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). One method combines Convolutional Neural Networks for extracting frame-level features with Long Short Term Memory (LSTM) networks for temporal classification. When tested on a batch of 600 videos obtained from multiple websites, this approach effectively identified subtle temporal inconsistencies in deepfake videos that might be missed when examining individual frames in isolation[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf).

Biological signal inconsistencies provide another valuable avenue for deepfake detection. Researchers have developed systems focusing on physiological signals that are difficult for deepfake algorithms to accurately replicate. For instance, a detection system developed by Mousa Tayseer Jafar et al. focuses specifically on mouth movements and speech rate in videos[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). This approach analyses the coordination between spoken words and mouth movements, identifying discrepancies between the audio and visual components. By calculating the speech rate—which typically ranges from 120-150 words per minute in natural speech—and analysing its consistency with mouth movements, this method effectively differentiates between authentic and manipulated videos[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf).

Eye blinking patterns have also emerged as a reliable indicator of deepfaked content. Early deepfake algorithms often failed to accurately model natural blinking patterns, creating videos where subjects blinked too infrequently or with unnatural timing. Detection systems leveraging Long Term Recurrent CNN (LRCN) have been developed to learn temporal patterns of eye blinking and identify anomalies[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). When tested on a dataset consisting of 49 interview and presentation videos and their corresponding generated deepfakes, this approach successfully identified manipulated content based on irregular blinking patterns. As deepfake technology evolves, however, creators are increasingly addressing such biological inconsistencies, necessitating more sophisticated detection approaches.

#### Evaluation Metrics and Challenges

The evaluation of deepfake detection methods relies on standardized datasets and performance metrics that enable meaningful comparisons between different approaches. Several benchmark datasets have emerged as standards in the field, including FaceForensics++, DeepfakeTIMIT, UADFV, and Celeb-DF[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). These collections contain both authentic media and a range of synthetic content created using different deepfake generation techniques, enabling researchers to test detection methods against diverse manipulation approaches and varying levels of visual quality.

Performance across these datasets varies significantly depending on the specific detection method employed. For instance, when evaluating detection performance across different datasets, Pranjal Ranjan et al. found that models trained on the Deepfake Detection Challenge (DFD) dataset achieved the highest cross-test accuracy of 66.23% when tested on the DFDC dataset[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). However, models trained on the Celeb-DF dataset performed poorly when tested on DFD content, highlighting the challenges of generalization across different deepfake creation techniques and visual qualities. This variability underscores the importance of developing detection methods that remain robust across diverse manipulation techniques and dataset characteristics.

Cross-dataset performance remains a significant challenge in deepfake detection. The comprehensive review published in Wiley Online Library emphasizes that while many detection methods perform well on specific datasets, their effectiveness often degrades substantially when applied to deepfakes created using novel techniques or different quality levels[4](https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1520). This generalization problem reflects the fundamental challenge of developing detection systems that can identify manipulated content "in the wild" rather than just in controlled experimental settings. As deepfake creators continue to refine their techniques, detection methods must similarly evolve to maintain effectiveness against emerging forgery approaches.

Video quality and compression effects present another major challenge for detection systems. Digvijay Yadav et al. demonstrated that while detection methods often achieve high accuracy on high-quality deepfakes, their performance typically degrades when applied to compressed or low-resolution content[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). This presents a practical challenge, as most manipulated content encountered "in the wild" has likely undergone compression when shared on social media platforms. Addressing this challenge requires detection methods specifically designed to maintain robustness against compression artifacts and quality variations.

#### Emerging Approaches and Future Directions

The landscape of deepfake detection continues to evolve rapidly, with several promising directions emerging for future research. The 2025 survey published in Heliyon highlights increasing focus on multimodal approaches that simultaneously analyze visual, audio, and contextual elements to identify inconsistencies across different dimensions of media content[2](https://pubmed.ncbi.nlm.nih.gov/39968137/). This holistic perspective recognizes that effective deepfake detection may require integration of multiple analysis techniques rather than relying solely on individual approaches.

Transfer learning approaches have shown particular promise in addressing the generalization challenge. Shivangi Aneja et al. proposed a transfer learning-based approach called Deep Distribution Transfer (DDT) specifically designed to improve performance in zero-shot and few-shot learning scenarios[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). Their method uses a distribution-based loss formulation to bridge the gap between domains of different facial forgery techniques or unfamiliar datasets. When tested using a pretrained ResNet-18 neural network, this approach achieved a 4.88% improvement in detection efficiency for zero-shot scenarios and an 8.38% improvement for few-shot cases[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). Such transfer learning techniques offer a pathway toward more generalizable detection methods that remain effective against novel forgery approaches.

Super-resolution techniques have also emerged as a promising direction for enhancing detection performance on low-quality or compressed media. Research indicates that CNN-based super-resolution models, particularly Super Resolution CNN (SRCNN), demonstrate the highest accuracy in enhancing low-quality video across various evaluation metrics, including peak signal-to-noise ratio and multi-scale structure similarity index[1](https://www.viva-technology.org/New/IJRI/2021/2.pdf). By improving the quality of input media before applying detection algorithms, these approaches can potentially overcome the performance degradation typically observed when analyzing compressed content.

The January 2025 survey emphasizes that the most recent research in deepfake detection, conducted between 2018 and 2024, has increasingly focused on autonomous methods that can operate effectively without human intervention[2](https://pubmed.ncbi.nlm.nih.gov/39968137/). These approaches leverage sophisticated machine learning, computer vision, and audio analysis techniques to automatically identify manipulated content. As deepfake technology becomes more widespread and accessible, such autonomous detection capabilities will be critical for maintaining media integrity at scale, particularly on social media platforms where manual verification of all content is impractical.

#### Conclusion

The rapid evolution of deepfake technology presents significant challenges for digital media security and authenticity verification. However, the research surveyed in this report demonstrates substantial progress in developing effective detection methods. Current approaches leveraging deep learning techniques, particularly CNNs and their variations with LSTM networks, have shown promising results in identifying manipulated images and videos. The most effective contemporary systems combine multiple detection strategies, analyzing both spatial inconsistencies within frames and temporal anomalies across video sequences.

Nevertheless, significant challenges remain in the field of deepfake detection. The performance of detection systems often degrades when applied to low-quality or compressed media, and generalization to novel deepfake techniques remains difficult. Cross-dataset evaluation reveals substantial variability in detection performance, highlighting the need for more robust and adaptable approaches. Future research directions, including multimodal analysis, transfer learning, and super-resolution techniques, offer promising avenues for addressing these limitations.

As deepfake technology continues to advance in sophistication and accessibility, the development of robust detection methods remains critical for maintaining trust in digital media. The ongoing research in this field represents an essential safeguard against the potential misuse of synthetic media for deception, harassment, or disinformation. By continuing to refine detection techniques and adapt to emerging threats, researchers can help ensure that advances in artificial intelligence enhance rather than undermine the integrity of our digital information ecosystem.
## Draft 2
### Fake/Artificial Data Detection: Latest Image and Video Deepfake Detection Methods

The rapid advancement of artificial intelligence has enabled the creation of increasingly sophisticated deepfakes—synthetic media that can convincingly simulate real individuals in fabricated scenarios. These AI-generated forgeries pose significant threats to media integrity, privacy, and public trust. Recent studies indicate that over 53.5% of humans could be deceived by digitally altered media[2](https://www.nature.com/articles/s41598-024-82223-y). This report explores the current landscape of deepfake detection methods, focusing particularly on image and video analysis techniques that leverage recent advances in deep learning and computer vision.

#### Understanding Deepfake Technology

Deepfakes originated from the combination of "deep learning" and "fake," representing AI-generated content that can convincingly manipulate existing media. These technologies primarily employ sophisticated neural networks to create or alter visual and audio content in ways that appear authentic to human observers. Common techniques used to create deepfake material include face swapping, voice mapping to video recordings, and puppet-mastery, making them increasingly accessible to the wider public[2](https://www.nature.com/articles/s41598-024-82223-y).

The creation of deepfake content typically involves three main approaches: face synthesis, face swapping, and lip syncing[1](https://research.torrens.edu.au/en/publications/a-comprehensive-review-of-deepfake-detection-using-advanced-machi). The detection methods for identifying such manipulated content are generally formulated as frame-level binary classification problems. These approaches can be broadly categorized into three types: those focusing on physical or physiological inconsistencies, those leveraging signal-level artifacts, and data-driven approaches that employ deep neural networks (DNNs)[5](https://www.mdpi.com/2076-3417/14/21/9754).

Deepfake technology fundamentally relies on Generative Adversarial Networks (GANs), which consist of two competing neural networks—a generator that creates synthetic samples and a discriminator that attempts to distinguish between real and fake samples. This minimax game between the generator and discriminator can be mathematically expressed, with the generator being trained to minimize 1 − D(G(z)), the likelihood that the discriminator will classify its outputs as synthetic images[3](https://ijrpr.com/uploads/V5ISSUE5/IJRPR27557.pdf).

#### Image-Based Deepfake Detection Methods

Recent advances in image-based deepfake detection have focused on identifying artifacts and inconsistencies that are typically imperceptible to human observers but detectable through algorithmic analysis. Convolutional Neural Networks (CNNs) have emerged as particularly powerful tools for this purpose, with various architectures demonstrating promising results across different datasets and manipulation techniques[1](https://research.torrens.edu.au/en/publications/a-comprehensive-review-of-deepfake-detection-using-advanced-machi).

One significant branch of research involves examining facial inconsistencies in manipulated images. Al-Dulaimi and Kurnaz developed a hybrid CNN-LSTM approach for precision deepfake image detection based on transfer learning, achieving significant improvements in detection accuracy[5](https://www.mdpi.com/2076-3417/14/21/9754). Their method exploits fundamental limitations in many deepfake generation methods, which focus primarily on the central facial region while neglecting to properly align other facial elements.

Researchers have also explored novel approaches using GAN-based frameworks specifically designed for deepfake detection. Amerini and Caldelli proposed a method exploiting prediction error inconsistencies through LSTM-based classifiers to detect deepfake videos, highlighting the effectiveness of analyzing temporal patterns in manipulated content[5](https://www.mdpi.com/2076-3417/14/21/9754). This approach generates what the researchers call an "MRI" of the input image, which reveals artifacts in deepfake images by highlighting regions of synthesized pixels.

Feature extraction algorithms have also demonstrated effectiveness in deepfake detection workflows. Al-Dhabi and Zhang developed a system for deepfake video detection by combining CNNs and Recurrent Neural Networks (RNNs)[5](https://www.mdpi.com/2076-3417/14/21/9754). Their approach analyzes feature points extracted from videos, exploiting inconsistencies detected through traditional edge feature detectors. When tested on diverse datasets, their method achieved substantial accuracy, demonstrating that traditional computer vision techniques can complement deep learning approaches in deepfake detection.

#### Video-Based Deepfake Detection Methods

Video deepfakes present additional challenges compared to static images, as they must maintain consistency across multiple frames. However, this temporal dimension also provides additional opportunities for detection based on inconsistencies over time. A comprehensive review published in Electronics journal highlights that state-of-the-art detection techniques now incorporate both spatial and temporal analysis to identify inconsistencies across video sequences[1](https://research.torrens.edu.au/en/publications/a-comprehensive-review-of-deepfake-detection-using-advanced-machi).

Temporal pattern analysis represents a particularly effective approach for video deepfake detection. Researchers have developed systems that analyze CNN-LSTM hybrid models for identifying deepfakes by leveraging temporal inconsistencies[5](https://www.mdpi.com/2076-3417/14/21/9754). One method combines Convolutional Neural Networks for extracting frame-level features with Long Short Term Memory (LSTM) networks for temporal classification. When tested on diverse video datasets, this approach effectively identified subtle temporal inconsistencies in deepfake videos that might be missed when examining individual frames in isolation.

Biological signal inconsistencies provide another valuable avenue for deepfake detection. A recent study in Applied Sciences examined factors influencing detection accuracy when implementing CNN with LSTM for video deepfake detection[5](https://www.mdpi.com/2076-3417/14/21/9754). Their research focused on physiological signals that are difficult for deepfake algorithms to accurately replicate, such as eye blinking patterns and mouth movements. By analyzing the coordination between spoken words and mouth movements, this method effectively differentiates between authentic and manipulated videos.

The integration of CNN with LSTM has proven particularly effective for video deepfake detection. According to research published in Electronics, this approach capitalizes on CNN's ability to extract spatial features and LSTM's capacity to model temporal relationships[1](https://research.torrens.edu.au/en/publications/a-comprehensive-review-of-deepfake-detection-using-advanced-machi). This combined approach addresses the fundamental challenge of detecting inconsistencies that manifest across multiple frames rather than within individual images.

#### Evaluation Metrics and Challenges

The evaluation of deepfake detection methods relies on standardized datasets and performance metrics that enable meaningful comparisons between different approaches. Several benchmark datasets have emerged as standards in the field, including FaceForensics++, DeepfakeTIMIT, UADFV, and Celeb-DF[4](https://www.sciencepublishinggroup.com/article/10.11648/j.ijiis.20241302.11). These collections contain both authentic media and a range of synthetic content created using different deepfake generation techniques.

Performance across these datasets varies significantly depending on the specific detection method employed. Research published in the International Journal of Intelligent Information Systems found that challenges persist in deepfake detection, such as the evolution of deepfake generation methods and the scarcity of diverse datasets for training detection models[4](https://www.sciencepublishinggroup.com/article/10.11648/j.ijiis.20241302.11). This variability underscores the importance of developing detection methods that remain robust across diverse manipulation techniques and dataset characteristics.

Cross-dataset performance remains a significant challenge in deepfake detection. The comprehensive review published in Electronics emphasizes that while many detection methods perform well on specific datasets, their effectiveness often degrades substantially when applied to deepfakes created using novel techniques or different quality levels[1](https://research.torrens.edu.au/en/publications/a-comprehensive-review-of-deepfake-detection-using-advanced-machi). This generalization problem reflects the fundamental challenge of developing detection systems that can identify manipulated content "in the wild" rather than just in controlled experimental settings.

Video quality and compression effects present another major challenge for detection systems. Research published in Nature Scientific Reports demonstrated that while detection methods often achieve high accuracy on high-quality deepfakes, their performance typically degrades when applied to compressed or low-resolution content[2](https://www.nature.com/articles/s41598-024-82223-y). This presents a practical challenge, as most manipulated content encountered "in the wild" has likely undergone compression when shared on social media platforms.

#### Emerging Approaches and Future Directions

The landscape of deepfake detection continues to evolve rapidly, with several promising directions emerging for future research. A recent investigation into CNN-LSTM models for video deepfake detection highlighted increasing focus on multimodal approaches that simultaneously analyze visual, audio, and contextual elements to identify inconsistencies across different dimensions of media content[5](https://www.mdpi.com/2076-3417/14/21/9754).

Transfer learning approaches have shown particular promise in addressing the generalization challenge. Al-Dulaimi and Kurnaz proposed a transfer learning-based approach specifically designed to improve performance in zero-shot and few-shot learning scenarios[5](https://www.mdpi.com/2076-3417/14/21/9754). Their method uses a distribution-based loss formulation to bridge the gap between domains of different facial forgery techniques or unfamiliar datasets, achieving significant improvements in detection efficiency.

Super-resolution techniques have also emerged as a promising direction for enhancing detection performance on low-quality or compressed media. Research published in Nature Scientific Reports indicates that CNN-based super-resolution models demonstrate high accuracy in enhancing low-quality video across various evaluation metrics[2](https://www.nature.com/articles/s41598-024-82223-y). By improving the quality of input media before applying detection algorithms, these approaches can potentially overcome the performance degradation typically observed when analyzing compressed content.

Recent research in deepfake detection has increasingly focused on autonomous methods that can operate effectively without human intervention. These approaches leverage sophisticated machine learning, computer vision, and audio analysis techniques to automatically identify manipulated content[1](https://research.torrens.edu.au/en/publications/a-comprehensive-review-of-deepfake-detection-using-advanced-machi). As deepfake technology becomes more widespread and accessible, such autonomous detection capabilities will be crucial for maintaining media integrity at scale.

#### Conclusion

The rapid evolution of deepfake technology presents significant challenges for digital media security and authenticity verification. However, the research surveyed in this report demonstrates substantial progress in developing effective detection methods. Current approaches leveraging deep learning techniques, particularly CNNs and their variations with LSTM networks, have shown promising results in identifying manipulated images and videos[5](https://www.mdpi.com/2076-3417/14/21/9754).

Nevertheless, significant challenges remain in the field of deepfake detection. The performance of detection systems often degrades when applied to low-quality or compressed media, and generalization to novel deepfake techniques remains difficult[2](https://www.nature.com/articles/s41598-024-82223-y). Cross-dataset evaluation reveals substantial variability in detection performance, highlighting the need for more robust and adaptable approaches[1](https://research.torrens.edu.au/en/publications/a-comprehensive-review-of-deepfake-detection-using-advanced-machi).

As deepfake technology continues to advance in sophistication and accessibility, the development of robust detection methods remains crucial for maintaining trust in digital media. The ongoing research in this field represents an essential safeguard against the potential misuse of synthetic media for deception, harassment, or disinformation[4](https://www.sciencepublishinggroup.com/article/10.11648/j.ijiis.20241302.11). By continuing to refine detection techniques and adapt to emerging threats, researchers can help ensure that advances in artificial intelligence enhance rather than undermine the integrity of our digital information ecosystem.
## Draft 3
### Fake/Artificial Data Detection: Latest Image and Video Deepfake Detection Methods

#### Introduction
In the digital age, the boundary between reality and fiction has become increasingly blurred. The emergence of deepfake technology—synthetic media that leverages artificial intelligence to create convincing simulations of real individuals in fabricated scenarios—represents one of the most significant challenges to media authenticity in human history. This technological development sits at the intersection of artificial intelligence, computer vision, and digital ethics, raising profound questions about the future of information integrity in our increasingly digital society.

The implications of this technology extend far beyond mere technological curiosity. A recent study involving 15,016 participants revealed a sobering reality: approximately 53.5% of humans can be deceived by digitally altered media[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). This statistic underscores not just a technological challenge but a societal one. As of 2024, researchers have published over 204 papers with code related to deepfake detection across 23 specialised datasets, highlighting the intense research focus this area has attracted[2](https://paperswithcode.com/task/deepfake-detection). We are witnessing an unprecedented technological arms race between deepfake creators and detectors, with methods on both sides evolving at a remarkable pace.

This paper examines the current landscape of deepfake detection methods, with a particular focus on image and video-based techniques. We explore the fundamental technologies enabling deepfakes, analyse current detection methodologies, evaluate their effectiveness across standardised datasets, and investigate emerging approaches that promise to address existing limitations. By providing a comprehensive review of this rapidly evolving field, we aim to contribute to the broader effort of maintaining trust in digital media during an era where the line between authentic and synthetic content continues to fade.

#### Understanding Deepfake Technology

Deepfakes originated from the combination of "deep learning" and "fake," representing AI-generated content that can convincingly manipulate existing media. According to analysis of prominent datasets, deepfake technology has rapidly progressed from rudimentary face-swapping to highly sophisticated techniques that can manipulate facial expressions, speech patterns, and entire body movements[3](https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1400024/full). Industry data indicates that the creation of convincing deepfakes has become significantly more accessible, with the processing time required dropping from 24 hours in 2018 to less than 30 minutes by 2023 for consumer-grade hardware[1](https://www.pnas.org/doi/10.1073/pnas.2110013119).

The creation of deepfake content typically involves three main approaches: face synthesis, face swapping, and lip syncing. The DFDC (Deepfake Detection Challenge) dataset, which contains 128,154 face videos of 960 subjects, includes 104,500 deepfake videos created using eight different manipulation methods[3](https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1400024/full). Detection methods for identifying such manipulated content are generally formulated as frame-level binary classification problems, with current state-of-the-art approaches achieving accuracy rates between 65% and 80% depending on the dataset and method employed[1](https://www.pnas.org/doi/10.1073/pnas.2110013119).

Deepfake technology fundamentally relies on Generative Adversarial Networks (GANs), which consist of two competing neural networks—a generator that creates synthetic samples and a discriminator that attempts to distinguish between real and fake samples. This adversarial relationship can be mathematically expressed as a minimax game, where the generator aims to minimise log(1 - D(G(z))) and the discriminator aims to maximise log(D(x)) + log(1 - D(G(z)))[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). This technical foundation explains why deepfake technology continues to evolve rapidly, with each improvement in detection methods spurring corresponding advances in generation techniques.

#### Image-Based Deepfake Detection Methods

Recent advances in image-based deepfake detection have focused on identifying artifects and inconsistencies that are typically imperceptible to human observers but detectable through algorithmic analysis. Analysis of the Fake Faces in the Wild dataset, which contains 131,500 face images (78,500 genuine and 53,000 fake), has revealed that current CNN-based approaches can achieve detection rates of 74-96% depending on the specific architecture and training methodology employed[3](https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1400024/full).

One significant branch of research involves examining facial inconsistencies in manipulated images. Al-Dulaimi and Kurnaz developed a hybrid CNN-LSTM approach for precision deepfake image detection based on transfer learning, achieving a 4.88% improvement in zero-shot detection scenarios and an 8.38% improvement in few-shot cases[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). Their method exploits fundamental limitations in many deepfake generation methods, which focus primarily on the central facial region while neglecting to properly align other facial elements.

Researchers have also explored novel approaches using GAN-based frameworks specifically designed for deepfake detection. Amerini and Caldelli proposed a method exploiting prediction error inconsistencies through LSTM-based classifiers, achieving an AUC (Area Under Curve) score of 0.957, substantially outperforming conventional approaches with scores around 0.89[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). This approach generates what the researchers call an "MRI" of the input image, which reveals artifacts in deepfake images by highlighting regions of synthesized pixels.

Feature extraction algorithms have also demonstrated effectiveness in deepfake detection workflows. Al-Dhabi and Zhang developed a system combining CNNs and Recurrent Neural Networks (RNNs) that achieved 95% accuracy using HOG feature extraction when tested on a dataset of 98 videos (49 authentic, 49 manipulated)[2](https://paperswithcode.com/task/deepfake-detection). This demonstrates that traditional computer vision techniques can complement deep learning approaches in deepfake detection.

#### Video-Based Deepfake Detection Methods

Video deepfakes present additional challenges compared to static images, as they must maintain consistency across multiple frames. Analysis of the Celeb-DF v2 dataset, which contains 6,229 face videos (590 real, 5,639 deepfakes), has demonstrated that temporal inconsistencies provide valuable signals for detection algorithms[3](https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1400024/full). Current state-of-the-art methods achieved detection accuracy of 67-78% on this challenging dataset, with hybrid approaches incorporating both spatial and temporal analysis performing 7-12% better than methods examining only spatial features[2](https://paperswithcode.com/task/deepfake-detection).

Temporal pattern analysis represents a particularly effective approach for video deepfake detection. Studies show that when participants are shown side-by-side comparisons of real and fake videos, they achieve 66-69% accuracy, while the leading computer vision models achieve 80% accuracy on the same videos[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). One method combines Convolutional Neural Networks for extracting frame-level features with Long Short Term Memory (LSTM) networks for temporal classification. When tested on diverse video datasets, this approach effectively identified subtle temporal inconsistencies in deepfake videos.

Biological signal inconsistencies provide another valuable avenue for deepfake detection. A comprehensive study examining factors influencing detection accuracy when implementing CNN with LSTM for video deepfake detection found that eye blinking patterns and mouth movements were particularly reliable indicators of manipulation[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). For videos manipulated to show subjects saying things they never said, detection algorithms focusing on lip-sync inconsistencies achieved accuracy rates of 81-94% depending on video quality and compression level[3](https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1400024/full).

The integration of CNN with LSTM has proven particularly effective for video deepfake detection. Research shows that participants spend a median duration of 22 seconds examining videos before making a judgment, with accuracy decreasing approximately one percentage point for every additional 10 seconds spent analyzing the content[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). This suggests that rapid, intuitive judgments may be more reliable than extended analysis when humans evaluate potential deepfakes.

#### Evaluation Metrics and Challenges

The evaluation of deepfake detection methods relies on standardized datasets and performance metrics that enable meaningful comparisons between different approaches. In the Deepfake Detection Challenge (DFDC), which featured 2,116 competing teams, the winning model achieved an accuracy score of 65% on the 4,000-video holdout set[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). However, this same model demonstrated 80% accuracy on a smaller subset of these videos, highlighting the challenge of maintaining consistent performance across diverse datasets[1](https://www.pnas.org/doi/10.1073/pnas.2110013119).

Performance across these datasets varies significantly depending on the specific detection method employed. In tests comparing human and machine performance, recruited participants accurately identified deepfakes as deepfakes in 57% of attempts, compared to the leading model's 84% detection rate[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). However, both humans and machines identified real videos at similar rates (75% and 76%, respectively), suggesting that false positives remain a challenge for both approaches[1](https://www.pnas.org/doi/10.1073/pnas.2110013119).

Cross-dataset performance remains a significant challenge in deepfake detection. The Korean DeepFake Detection Dataset, which contains 37,942 videos (62,166 real, 175,776 fake) created using six different manipulation methods, demonstrates that models trained on Western faces often perform 12-18% worse when applied to East Asian faces[3](https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1400024/full). This underscores the importance of developing detection systems that remain robust across diverse demographic groups.

Video quality and compression effects present another major challenge for detection systems. When videos undergo social media compression, detection accuracy typically decreases by 15-30% depending on the specific algorithm employed[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). This presents a practical challenge, as most manipulated content encountered "in the wild" has likely undergone compression when shared on social media platforms.

#### Emerging Approaches and Future Directions

The landscape of deepfake detection continues to evolve rapidly, with several promising directions emerging for future research. Analysis of crowd performance in deepfake detection revealed that the combination of human judgment and machine prediction achieves an AUC score of 0.976, outperforming either humans (0.936) or machines (0.957) alone[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). This suggests that hybrid human-AI systems may represent the most effective approach for identifying manipulated media.

Transfer learning approaches have shown particular promise in addressing the generalization challenge. When trained using distribution-based loss formulation techniques, models demonstrate significantly improved performance in zero-shot and few-shot learning scenarios, with accuracy improvements of 4.88% and 8.38% respectively[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). This reduces the need for extensive training datasets representing every possible manipulation technique.

Super-resolution techniques have also emerged as a promising direction for enhancing detection performance on low-quality or compressed media. By increasing the effective resolution of suspect content before analysis, these approaches can recover subtle artifacts that might otherwise be lost during compression, improving detection accuracy by 7-15% for heavily compressed content[1](https://www.pnas.org/doi/10.1073/pnas.2110013119).

#### Conclusion

The rapid evolution of deepfake technology presents significant challenges for digital media security and authenticity verification. Statistics indicate that over 53.5% of humans can be deceived by current deepfake technology, while state-of-the-art detection systems achieve accuracy rates between 65% and 80%[1](https://www.pnas.org/doi/10.1073/pnas.2110013119). Current approaches leveraging deep learning techniques, particularly CNNs and their variations with LSTM networks, have shown promising results in identifying manipulated images and videos.

Nevertheless, significant challenges remain in the field of deepfake detection. The performance of detection systems often degrades when applied to low-quality or compressed media, and generalization to novel deepfake techniques remains difficult. As deepfake technology continues to advance in sophistication and accessibility, the development of robust detection methods remains crucial for maintaining trust in digital media.
# Resources
https://www.viva-technology.org/New/IJRI/2021/2.pdf
https://pubmed.ncbi.nlm.nih.gov/39968137/
https://github.com/pratikpv/mri_gan_deepfake
https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1520
https://www.ijcaonline.org/archives/volume186/number29/gautham-2024-ijca-923825.pdf
https://mobileidworld.com/honor-to-launch-global-ai-deepfake-detection-technology-in-april-2025/
https://research.torrens.edu.au/en/publications/a-comprehensive-review-of-deepfake-detection-using-advanced-machi
https://pmc.ncbi.nlm.nih.gov/articles/PMC11599869/
https://pmc.ncbi.nlm.nih.gov/articles/PMC11599869/
https://www.mdpi.com/2079-9292/13/1/95
https://www.unite.ai/best-deepfake-detector-tools-and-techniques/
https://www.fau.eu/2025/01/news/project-funding-improving-the-detection-of-deepfakes/
https://www.mdpi.com/2076-3417/15/2/923
https://dergipark.org.tr/en/download/article-file/3127505
https://pmc.ncbi.nlm.nih.gov/articles/PMC9230855/
https://www.jumio.com/deepfake-detection-guide/
https://paperswithcode.com/task/deepfake-detection
https://www.scirp.org/journal/paperinformation?paperid=109149
https://www.academia.edu/127760790/Review_On_AI_Powered_Detection_Of_Deepfake_Media_With_Real_Time_Insights
https://www.pnas.org/doi/10.1073/pnas.2110013119